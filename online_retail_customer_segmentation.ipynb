{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450b9c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eef3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Shu   Online_Retail_Customer_Segmentation_Capstone_Project.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1wsjkMlJBcsEFL2b6C3qSuTsv3QCZjclG\n",
    "\n",
    "#<font color = red> <b><u> Project Title : Extraction/identification of major topics & themes discussed in news articles. </u></b>\n",
    "\n",
    "##<font color = red> <b> Problem Description </b>\n",
    "\n",
    "###<font color = green> In this project, your task is to identify major customer segments on a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "##<font color = red> <b> Data Description </b>\n",
    "\n",
    "###<font color = green> <b>Attribute Information: </b>\n",
    "\n",
    "* ### InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "* ### StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "* ### Description: Product (item) name. Nominal.\n",
    "* ### Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "* ### InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "* ### UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
    "* ### CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "* ### Country: Country name. Nominal, the name of the country where each customer resides.\n",
    "\"\"\"\n",
    "\n",
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "#imorting important libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "# setting max display column limit to 30\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy import math\n",
    "\n",
    "#mounting google drive for import the dataset\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "#loading the dataset\n",
    "df=pd.read_csv('/content/drive/MyDrive/1.Project/Customer Segmentation/Online Retail.csv',encoding=\"ISO-8859-1\")\n",
    "\n",
    "\"\"\"# <font color='Green'>**Getting insight from data**\"\"\"\n",
    "\n",
    "# top 5 rows\n",
    "df.head()\n",
    "\n",
    "#shape of dataset\n",
    "print(df.shape)\n",
    "\n",
    "# Data set columns\n",
    "df.columns\n",
    "\n",
    "# the unique value for the dataset\n",
    "df.nunique()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "\"\"\"# <font color=red> <b> **Data Wrangling:**\"\"\"\n",
    "\n",
    "df.info()\n",
    "\n",
    "\"\"\"###<font color=Green> **Observations**\n",
    "* ***Datatype of InvoiceDate is object need to convert it into datatime.***\n",
    "* ***If InvoiceNo starts with C means it's a cancellation. We need to drop this entries.***\n",
    "* ***There are null values in CustomerID and Description.***\n",
    "\"\"\"\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "\"\"\"# <font color=red>**Observations**\n",
    "### <font color=green>***Missing values in CustomerID and Description columns.***\n",
    "###<font color=green> ***CustomerID is our identification feature so if its missing means other wont help us in analysis***\n",
    "###<font color=green> ***Dropping that all missing datapoints***\n",
    "\"\"\"\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.shape\n",
    "\n",
    "df.describe()\n",
    "\n",
    "# we can see Invoice number starting with C has negative values and as per description of the data those are cancelations so we need to drop this entries.\n",
    "df[df['Quantity']<0].head(3)\n",
    "\n",
    "# changing the datatype to str\n",
    "df['InvoiceNo'] = df['InvoiceNo'].astype('str')\n",
    "# also If InvoiceNo starts with C means it's a cancellation droping this entries.\n",
    "df=df[~df['InvoiceNo'].str.contains('C')]\n",
    "\n",
    "# Checking how many values are present for unitprice ==0\n",
    "# almost 40 values are present so will drop this values\n",
    "print(len(df[df['UnitPrice']==0]))\n",
    "\n",
    "# taking unitprice values greater than 0.\n",
    "df=df[df['UnitPrice']>0]\n",
    "df.head()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "df.shape\n",
    "\n",
    "\"\"\"# <font color=red>**Feature Engineering**\"\"\"\n",
    "\n",
    "df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\n",
    "\n",
    "df['year'] = df['InvoiceDate'].dt.year\n",
    "df['month_num'] = df['InvoiceDate'].dt.month\n",
    "df['day_num'] = df['InvoiceDate'].dt.day\n",
    "df['hour'] = df['InvoiceDate'].dt.hour\n",
    "df['minute'] = df['InvoiceDate'].dt.minute\n",
    "\n",
    "# extracting month from the Invoice date\n",
    "df['Month']=df['InvoiceDate'].dt.month_name()\n",
    "# extracting day from the Invoice date\n",
    "df['Day']=df['InvoiceDate'].dt.day_name()\n",
    "\n",
    "df['TotalAmount']=df['Quantity']*df['UnitPrice']\n",
    "\n",
    "df.head()\n",
    "\n",
    "\"\"\"##<font color =red > <b> Exploratory Data Analysis (EDA) </B>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df.columns\n",
    "\n",
    "top_10_product=df['Description'].value_counts().reset_index().rename(columns={'index':'Product_name','Description':'Count'}).head(10)\n",
    "top_10_product\n",
    "\n",
    "\"\"\"##<font color =Green > ***Top 10 items in terms of description(Name)***\"\"\"\n",
    "\n",
    "# top 10 product in terms of description\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=top_10_product['Count'],y=top_10_product['Product_name'])\n",
    "plt.title('Top 10 products')\n",
    "\n",
    "\"\"\"###<font color =Green > **Observations**\n",
    "* ***WHITE HANGING HEART T-LIGHT HOLDER\tis the highest selling product almost 2018 units were sold***\n",
    "* ***REGENCY CAKESTAND 3 TIER is the 2nd highest selling product almost 1723 units were sold***\n",
    "\n",
    "## <font color =Blue >***Bottom 10 items in terms of description(Name)***\n",
    "\"\"\"\n",
    "\n",
    "bottom_10_product=df['Description'].value_counts().reset_index().rename(columns={'index':'Product_name','Description':'Count'}).tail(10)\n",
    "bottom_10_product\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"##<font color = Blue > ***Top 10 items in terms of StockCode.***\"\"\"\n",
    "\n",
    "top_10_StockCodes=df['StockCode'].value_counts().reset_index().rename(columns={'index':'StockCode','StockCode':'Count'}).head(10)\n",
    "top_10_StockCodes\n",
    "\n",
    "# top 10 product in terms of StcokCode\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=top_10_StockCodes['Count'],y=top_10_StockCodes['StockCode'])\n",
    "plt.title('Top 10 products in terms of Stock Code')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# <font color =Green >**Observations**\n",
    "* ***StockCode-85123Ais the first highest selling product.***\n",
    "* ***StockCode-22423 is the 2nd highest selling product.***\n",
    "\"\"\"\n",
    "\n",
    "top_10_customers=df['CustomerID'].value_counts().reset_index().rename(columns={'index':'CustomerID','CustomerID':'Products_purchasing_Count'}).head(10)\n",
    "# Top 10 frequent Customers\n",
    "plt.figure(figsize=(18,6))\n",
    "sns.barplot(x=top_10_customers['CustomerID'],y=top_10_customers['Products_purchasing_Count'].head(10))\n",
    "plt.title('Top 10 frequent Customers.')\n",
    "\n",
    "\"\"\"# <font color =Green >**Observations**\n",
    "* ***CustomerID- 17841 had purchased highest number of products***\n",
    "* ***CustomerID-14911 is the 2nd higest customer who purchased the most the products***\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "top_5_countries=df['Country'].value_counts().reset_index().rename(columns={'index':'Country','Country':'Customer_count'})\n",
    "top_5_countries\n",
    "# top 5 countries where max sell happens.\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(x=top_5_countries['Country'].head(5),y=top_5_countries['Customer_count'].head(5))\n",
    "plt.title('Top 5 Countries based on highest number of customers')\n",
    "\n",
    "\"\"\"# <font color =Green >**Observations**\n",
    "* ***UK has highest number of customers***\n",
    "* ***Germany,France and IreLand has almost equal number of customers***\n",
    "\"\"\"\n",
    "\n",
    "# top 5 countries where max sell happens.\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(x=top_5_countries['Country'].tail(5),y=top_5_countries['Customer_count'].tail(5))\n",
    "plt.title('Top 5 Countries based on least number of customers ')\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# <font color =Green >**Observations**\n",
    "* ***There are very less customers from Saudi Arabia***\n",
    "* ***Bahrain is the 2nd Country having least number of customers***\n",
    "\"\"\"\n",
    "\n",
    "sales_in_month=df['Month'].value_counts().reset_index().rename(columns={'index':'Month','Month':'Sales_count'})\n",
    "# Sales count in different months.\n",
    "plt.figure(figsize=(20,6))\n",
    "sns.barplot(x=sales_in_month['Month'],y=sales_in_month['Sales_count'])\n",
    "plt.title('Sales count in different Months ')\n",
    "\n",
    "\"\"\"# <font color =Green >**Observations**\n",
    "* ***Most of the sale happened in Novmenber month.***\n",
    "* ***December Month had least sales.***\n",
    "\"\"\"\n",
    "\n",
    "sales_on_day_basis=df['Day'].value_counts().reset_index().rename(columns={'index':'Day',\"Day\":'Sale_count'})\n",
    "# Sales count on different days.\n",
    "plt.figure(figsize=(20,6))\n",
    "sns.barplot(x=sales_on_day_basis['Day'],y=sales_on_day_basis['Sale_count'])\n",
    "plt.title('Sales count on different Days ')\n",
    "\n",
    "\"\"\"# <font color =Green >**Observations**\n",
    "* ***Sales On Thursdays are very high.***\n",
    "* ***Sales On Saturday are very less***\n",
    "\"\"\"\n",
    "\n",
    "df['hour'].unique()\n",
    "\n",
    "def time(time):\n",
    "  if (time==6 or time==7 or time==8 or time==9 or time==10 or time==11) :\n",
    "    return'Morning'\n",
    "  elif (time==12 or time==13 or time==14 or time==15 or time==16 or time==17):\n",
    "    return 'Afternoon'\n",
    "  else:\n",
    "    return 'Evening'\n",
    "\n",
    "df['Day_time_type']=df['hour'].apply(time)\n",
    "\n",
    "sales_timing=df['Day_time_type'].value_counts().reset_index().rename(columns={'index':'Day_time_type','Day_time_type':'Sales_count'})\n",
    "# Sales count on different days.\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=sales_timing['Day_time_type'],y=sales_timing['Sales_count'])\n",
    "plt.title('Sales count in different day timings')\n",
    "\n",
    "\"\"\"# <font color =Green >**Observations**\n",
    "* ***Most of the sales happens in the afternoon.***\n",
    "* ***Least sales happens in the evening.***\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "avg_amount=df.groupby('CustomerID')['TotalAmount'].mean().reset_index().rename(columns={'TotalAmount':'Avg_amount_per_customer'}).sort_values('Avg_amount_per_customer',ascending=False)\n",
    "# Sales count on different days.\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(x=avg_amount['CustomerID'].head(5),y=avg_amount['Avg_amount_per_customer'].head(15))\n",
    "plt.title('Average amount spent by each Customer')\n",
    "\n",
    "\"\"\"# <font color =Green >**Observations**\n",
    "* ***77183 (Dollars)is the highest average amount spent by the CustomerID-12346***\n",
    "* ***56157 (Dollars) is the 2nd  highest average amount spent by the CustomerID-16446***\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# <font color = Red > <b > Model Building\n",
    "\n",
    "# <font color = Orange > **RFM Model Analysis:**\n",
    "\n",
    "#<font color = Red > **Why it is Needed?**\n",
    "<font color = Green >***RFM Analysis is a marketing framework that is used to understand and analyze customer behaviour based on the above three factors RECENCY, Frequency, and Monetary.***\n",
    "\n",
    "<font color = Green >***The RFM Analysis will help the businesses to segment their customer base into different homogenous groups so that they can engage with each group with different targeted marketing strategies.***\n",
    "\"\"\"\n",
    "\n",
    "rfm_df=df.copy()\n",
    "\n",
    "rfm_df.head()\n",
    "\n",
    "import datetime as dt\n",
    "#Set Latest date 2011-12-10 as last invoice date was 2011-12-09. This is to calculate the number of days from recent purchase\n",
    "Latest_Date = dt.datetime(2011,12,10)\n",
    "#Create RFM Modelling scores for each customer\n",
    "rfm_df = df.groupby('CustomerID').agg({'InvoiceDate': lambda x: (Latest_Date - x.max()).days, 'InvoiceNo': lambda x: len(x), 'TotalAmount': lambda x: x.sum()})\n",
    "#Convert Invoice Date into type int\n",
    "rfm_df['InvoiceDate'] = rfm_df['InvoiceDate'].astype(int)\n",
    "rfm_df.rename(columns={'InvoiceDate': 'Recency', \n",
    "                         'InvoiceNo': 'Frequency', \n",
    "                         'TotalAmount': 'Monetary'}, inplace=True)\n",
    "rfm_df.reset_index().head()\n",
    "\n",
    "# Descriptive Stats= Recency\n",
    "rfm_df.Recency.describe()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(x=rfm_df['Recency'])\n",
    "plt.title('Distribution of Recency')\n",
    "\n",
    "\n",
    "\n",
    "# Descriptive Stats= Frequency\n",
    "rfm_df['Frequency'].describe()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(x=rfm_df['Frequency'])\n",
    "plt.title('Distribution of Frequency')\n",
    "\n",
    "# Descriptive Stats= Monetary\n",
    "rfm_df['Monetary'].describe()\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(x=rfm_df['Monetary'])\n",
    "plt.title('Distribution of Monetary')\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into four segment using Quantile\n",
    "quantile = rfm_df.quantile(q = [0.25,0.50,0.75])\n",
    "quantile = quantile.to_dict()\n",
    "print(quantile)\n",
    "\n",
    "\n",
    "\n",
    "#Functions to create R, F and M segments\n",
    "def RScoring(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 1                               \n",
    "    elif x <= d[p][0.50]:\n",
    "        return 2\n",
    "    elif x <= d[p][0.75]: \n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "def FnMScoring(x,p,d):\n",
    "    if x <= d[p][0.25]:\n",
    "        return 4\n",
    "    elif x <= d[p][0.50]:\n",
    "        return 3\n",
    "    elif x <= d[p][0.75]: \n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Calculating and adding R,F and M segments values columns in the existing dataset to show R,F,M segment values\n",
    "rfm_df[\"R\"] = rfm_df['Recency'].apply(RScoring,args=('Recency',quantile,))\n",
    "rfm_df[\"F\"] = rfm_df['Frequency'].apply(FnMScoring,args=('Frequency',quantile,))\n",
    "rfm_df[\"M\"] = rfm_df['Monetary'].apply(FnMScoring,args=('Monetary',quantile,))\n",
    "rfm_df.head()\n",
    "\n",
    "# Add a new column to combine RFM score\n",
    "rfm_df['RFM_Group'] = rfm_df.R.map(str)+rfm_df.F.map(str)+rfm_df.M.map(str)\n",
    "#Calculate and Add RFMScore value column showing total sum of RFMGroup values\n",
    "rfm_df['RFM_Score'] = rfm_df[['R', 'F', 'M']].sum(axis = 1)\n",
    "rfm_df.head()\n",
    "\n",
    "\n",
    "\n",
    "# Assign Loyelty Level to each customer\n",
    "Loyalty_Level = ['Platinaum','Gold','Silver','Bronz']\n",
    "Score_cut = pd.qcut(rfm_df['RFM_Score'],q = 4,labels=Loyalty_Level)\n",
    "rfm_df['RFM_Loyalty_Level'] = Score_cut.values\n",
    "rfm_df.reset_index().head()\n",
    "\n",
    "\n",
    "\n",
    "# Validate the data For RFM group = 111\n",
    "rfm_df[rfm_df['RFM_Group'] == '111'].sort_values(\"Monetary\",ascending = False).reset_index().head(10)\n",
    "\n",
    "\n",
    "\n",
    "# Plot the loyelty level\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.countplot(rfm_df['RFM_Loyalty_Level'])\n",
    "plt.title('Loyalty Level of Customers')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Folowing table contains our target peoples\n",
    "rfm_df[rfm_df['RFM_Loyalty_Level'] == 'Platinaum'].sort_values(\"Monetary\",ascending = False).reset_index().head(10)\n",
    "segmentation_based_on_RFM=rfm_df[['Recency','Frequency','Monetary','RFM_Loyalty_Level']]\n",
    "segmentation_based_on_RFM.groupby('RFM_Loyalty_Level').agg({\n",
    "    'Recency': ['mean', 'min', 'max'],\n",
    "    'Frequency': ['mean', 'min', 'max'],\n",
    "    'Monetary': ['mean', 'min', 'max','count']\n",
    "})\n",
    "\n",
    "#Handle negative and zero values so as to handle infinite numbers during log transformation\n",
    "def handle_neg_n_zero(num):\n",
    "    if num <= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return num\n",
    "#Apply handle_neg_n_zero function to Recency and Monetary columns \n",
    "rfm_df['Recency'] = [handle_neg_n_zero(x) for x in rfm_df.Recency]\n",
    "rfm_df['Monetary'] = [handle_neg_n_zero(x) for x in rfm_df.Monetary]\n",
    "\n",
    "#Perform Log transformation to bring data into normal or near normal distribution\n",
    "Log_rfm_df = rfm_df[['Recency', 'Frequency', 'Monetary']].apply(np.log, axis = 1).round(3)\n",
    "\n",
    "# Now let's Visualize the Distribution of Recency,Frequency and Monetary.\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(x=Log_rfm_df['Recency'])\n",
    "plt.title('Distribution of Recency')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(x=Log_rfm_df['Frequency'])\n",
    "plt.title('Distribution of Frequency')\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(x=Log_rfm_df['Monetary'])\n",
    "plt.title('Distribution of Monetary')\n",
    "\n",
    "\n",
    "\n",
    "rfm_df['Recency_log'] = rfm_df['Recency'].apply(math.log)\n",
    "rfm_df['Frequency_log'] = rfm_df['Frequency'].apply(math.log)\n",
    "rfm_df['Monetary_log'] = rfm_df['Monetary'].apply(math.log)\n",
    "rfm_df.head(5)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# <font color =red > <b> 1 ) K Means Clustering : </b>\"\"\"\n",
    "\n",
    "#Importing Libraries\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "!pip install yellowbrick\n",
    "\n",
    "\"\"\"<font color = Green >Before implementing the Kmeans Clustering alogorthm we need to decide the number of clusters to put inside algorithm as input. So we will be finding the minmum number of clusters required by using Elbow method.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"# <font color =Blue > <b> Recency and Monetary.\n",
    "\n",
    "##<font color =Green > <b> A)  Applying Elbow Method on Recency and Monetary .\n",
    "\"\"\"\n",
    "\n",
    "# taking Recency and Monetory_log in list.\n",
    "Recency_and_Monetary_feat=['Recency_log','Monetary_log']\n",
    "# taking only values of recency and monetory in X.\n",
    "X=rfm_df[Recency_and_Monetary_feat].values\n",
    "# standardising the data\n",
    "scaler=StandardScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "\n",
    "#applying Elbow Method\n",
    "wcss = {}\n",
    "for k in range(1,15):\n",
    "    km = KMeans(n_clusters= k, init= 'k-means++', max_iter= 1000)\n",
    "    km = km.fit(X)\n",
    "    wcss[k] = km.inertia_\n",
    "#Plot the graph for the sum of square distance values and Number of Clusters\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.pointplot(x = list(wcss.keys()), y = list(wcss.values()))\n",
    "plt.xlabel('Number of Clusters(k)')\n",
    "plt.ylabel('Sum of Square Distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"###**Here we can see that Optimal value for cluster came out to be 2.**\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"### <font color =Blue > ***Cross verifying with ElbowVisualizar***\n",
    "The elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the elbow of the curve as the number of clusters to use.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(\n",
    "    model, k=(2,12), metric='calinski_harabasz', timings=False\n",
    ")\n",
    "visualizer.fit(X)        # Fit the data to the visualizer\n",
    "visualizer.show()        # Finalize and render the figure\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"## <font color =Blue > **Silhouette Score** ***(Validating Above optimal cluster value(i.e optimal_cluster=2)***\"\"\"\n",
    "\n",
    "# taking Recency and Monetory_log in list.\n",
    "Recency_and_Monetary_feat=['Recency_log','Monetary_log']\n",
    "# taking only values of recency and monetory in X.\n",
    "X=rfm_df[Recency_and_Monetary_feat].values\n",
    "# standardising the data\n",
    "scaler=StandardScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "#Silhouette Score\n",
    "range_n_clusters = [2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters,random_state=1)\n",
    "    preds = clusterer.fit_predict(X)\n",
    "    centers = clusterer.cluster_centers_\n",
    "\n",
    "    score = silhouette_score(X, preds)\n",
    "    print(\"For n_clusters = {}, silhouette score is {}\".format(n_clusters, score))\n",
    "\n",
    "\"\"\"***Here we can see the for n_cluster=2 silhouette score is good as compared to others.(if values is close to 1 means data points are clustered very well to respective clusters and distance of that datapoint is very far from the other cluster.)***\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "range_n_clusters = [2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=1)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) /n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0f188e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e56ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48542d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c1b756",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3022cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c20232c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805f181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6e18fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc9c329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bf4382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea0e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fedb11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d5126",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9e79c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942f6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f57375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a96c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58208b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a0390d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14c824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0540ef26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6c2fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
